---
title: "04_Field_Experiment_Appendix"
author: "Matt"
date: "9/16/2020"
output: html_document
---

This document will serve as the data wrangling portion for the failed field experiment portion of my Masters thesis. First, load the packages that will be used in this script

```{r Load packages}

suppressMessages(suppressWarnings({
  ls <- c("tidyverse", "car", "extrafont", "foreach", "ggpubr", "rstatix")
  new_packages <- ls[!(ls %in% installed.packages()[, "Package"])]
  if(length(new_packages)) install.packages(new_packages)
  sapply(unique(c(ls, tidyverse::tidyverse_packages())), library, character.only = TRUE)[0]
  if(any(!fonts() %in% unlist(windowsFonts())) || length(fonts()) == 0) {
    font_import(prompt = FALSE)
    loadfonts(device = "win")}
  rm(ls, new_packages)}))

```

Next, set the directories for each of the folders for inputs and outputs. Read in the field data spreadsheet.

```{r Set directories and load data}

data_dir <- file.path("./04_Field_Experiment_Appendix/data")
plot_dir <- file.path("./04_Field_Experiment_Appendix/plots")
results_dir <- file.path("./04_Field_Experiment_Appendix/results")
dir.create(plot_dir, showWarnings = FALSE)
dir.create(results_dir, showWarnings = FALSE)

field <- read.csv(file.path(data_dir, "FieldExp.csv")) %>% 
  dplyr::mutate(Treatment = sub("-", "_", Treatment))

```

The data from the field trials have two time points: Planting and Harvesting. Records for number of leaves of each individual and their respective leaf height were used for a two way repeated measures ANOVA (further below). Before doing that though, the other columns include changes in leaf height and number of leaves, and biomass collected. It's important to note here that in some cases, normality was violated for some of the treatment groups, but the analysis still proceeded. I made an arbitrary decision to allow an ANOVA if at least 5/7 of the treatments were normal, though this can change if necessary.

```{r anova on biomass and change variables}

##### ALTERNATIVE: USE rstatix package
vars <- c("Change_Height", "Change_Leaves", "Biomass")

one_way <- lapply(seq_along(vars), function(x) {
  # Normality - if 5/7 groups pass then pass
  y <- dplyr::rename(field, val = vars[x]) %>% 
    dplyr::select(Treatment, val)
  
  norm <- shapiro_test(residuals(lm(val ~ Treatment, data = y)))
  
  # If not normal, log transform
  if(norm$p.value <= 0.05) {
    z <- dplyr::mutate(y, val = case_when(
      min(val) <= 0 ~ log10(val + abs(min(val)) + 1),
      TRUE ~ log10(val)))
    
    norm <- shapiro_test(residuals(lm(val ~ Treatment, data = z)))
    
    # If still not normal, square root transform
    if(norm$p.value <= 0.05) {
      z <- dplyr::mutate(y, val = case_when(
        min(val) < 0 ~ sqrt(val + abs(min(val))), 
        TRUE ~ sqrt(val)))
      norm <- shapiro_test(residuals(lm(val ~ Treatment, data = z)))
      
      # If still not normal, square transform
      if(norm$p.value <= 0.05) {
        z <- dplyr::mutate(y, val = val ^ 2)
        norm <- shapiro_test(residuals(lm(val ~ Treatment, data = z)))
      }
    }
  } else z <- y
  
  # If still not normal, run Kruskal Wallis test on untransformed data with 
  # pairwise comparisons. Not sure which is better to use (dunn vs wilcox)
  if(norm$p.value <= 0.05) {
    kruskal <- kruskal_test(val ~ Treatment, data = y) %>% 
      dplyr::mutate(test = "Kruskal")
    pwc_dunn <- dunn_test(val ~ Treatment, data = y, p.adjust.method = "bonferroni") %>% 
      dplyr::mutate(test = "Dunn")
    pwc_wilcox <- wilcox_test(val ~ Treatment, data = y, p.adjust.method = "bonferroni") %>% 
      dplyr::mutate(test = "Wilcox")
    return(list(
      data = {dplyr::rename(y, !!vars[x] := val)}, 
      norm = {group_by(y, Treatment) %>% shapiro_test(val) %>% 
          dplyr::mutate(normal = p > 0.05, method = "kruskal")}, 
      kruskal = kruskal, pairwise_dunn = pwc_dunn, pairwise_wilcox = pwc_wilcox))
  } else {
    # Homogeneity of variances
    variance <- levene_test(val ~ Treatment, data = z) %>% 
      dplyr::mutate(homogenous = p > 0.05, test = "Levene")
    if(variance$homogenous) {
      anova <- data.frame(anova_test(val ~ Treatment, data = z)) %>% 
        dplyr::mutate(test = "ANOVA")
      tukey <- tukey_hsd(z, val ~ Treatment) %>% 
        dplyr::mutate(test = "Tukey")
      welch <- NULL
      g_h <- NULL
    } else {
      anova <- NULL
      tukey <- NULL
      welch <- welch_anova_test(val ~ Treatment, data = z) %>% 
        dplyr::mutate(test = "Welch ANOVA")
      g_h <- games_howell_test(val ~ Treatment, data = z) %>% 
        dplyr::mutate(test = "Games Howell")
    }
    return(Filter(Negate(is.null), list(
      data = {dplyr::rename(z, !!paste0(norm$method[1], "_", vars[x]) := val)}, 
      norm = norm, variance = variance, anova = anova, welch = welch, 
      tukey = tukey, games_howell = g_h)))
  }
}) %>% magrittr::set_names(vars)

# Write outputs
lapply(vars, function(x) {
  v <- one_way[[x]]
  file_out <- file.path(results_dir, paste0(x, "_Analysis.csv"))
  for(i in names(v)) {
    if(i == names(v)[1]) unlink(file_out)
    out <- v[[i]]
    out[nrow(out) + 1, ] <- NA
    write.table(out, file_out, append = TRUE, sep = ",", 
                row.names = FALSE, na = "")}})

```

The next part will involve transforming the original field dataset to allow the repeated measures anova to run as expected for leaves and height separately.

```{r Two way Repeated Measures data prep}

leaves <- dplyr::select(field, Treatment, Rep, Planting_Leaves, Harvest_Leaves) %>% 
  pivot_longer(c(Planting_Leaves, Harvest_Leaves)) %>% 
  dplyr::mutate(name = factor(name, levels = c("Planting_Leaves", "Harvest_Leaves")))
height <- dplyr::select(field, Treatment, Rep, Planting_Height, Harvest_Height) %>% 
  pivot_longer(c(Planting_Height, Harvest_Height)) %>% 
  dplyr::mutate(name = factor(name, levels = c("Planting_Height", "Harvest_Height")))

ls <- list(leaves = leaves, height = height)

```

Normality is violated in any scenario because of the 0's in the harvesting dataset. Because this experiment had poor results, I chose to use the repeated measures anova anyways. Of the data that is not 0 and that are not outliers, it ends up falling within the normal boundaries, which can be seen here:

```{r Normality plots}

ggqqplot(leaves, "value", ggtheme = theme_bw()) +
    facet_grid(name ~ Treatment, labeller = "label_both")
ggqqplot(height, "value", ggtheme = theme_bw()) +
    facet_grid(name ~ Treatment, labeller = "label_both")

```

Proceeding with a two way repeated measures ANOVA. I originally tried removing outliers from the analysis but then R wasn't able to run a sphericity test because the dataset was unbalanced. The code for removing the outliers is commented out here because of that.

```{r two way repeated measures ANOVA}

# Do some similar things from previously in case I delete it
rep_anova <- lapply(ls[!endsWith(names(ls), "nozero")], function(x) {
  # First, check outliers
  outliers <- group_by(x, Treatment, name) %>%
    identify_outliers(value) %>% 
    dplyr::filter(is.extreme)
  
  # If outliers are present, remove them
  # if(nrow(outliers) > 0) {
  #   x <- anti_join(x, outliers, by = c("Treatment", "Rep"))
  # }
  
  # Normality assumption requires at least 3 and 2 unique values to compute
  shapiro <- group_by(x, Treatment, name) %>% 
    mutate(n = length(unique(value))) %>% 
    # ungroup(name) %>% 
    # mutate(n = ifelse(any(n == 1), 1, n)) %>% 
    # group_by(name, .add = TRUE) %>% 
    dplyr::filter(n > 1) %>% 
    dplyr::select(-n) %>% 
    dplyr::rename(val = value) %>% 
    shapiro_test(val) %>% 
    dplyr::mutate(normal = p > 0.05)
  
  shapiro <- mutate(x, n = length(unique(value))) %>% 
    # ungroup(name) %>% 
    # mutate(n = ifelse(any(n == 1), 1, n)) %>% 
    # group_by(name, .add = TRUE) %>% 
    dplyr::filter(n > 1) %>% 
    dplyr::select(-n) %>% 
    dplyr::rename(val = value) %>% 
    shapiro_test(val) %>% 
    dplyr::mutate(normal = p > 0.05)
  
  norm_plot <- ggqqplot(x, "value", ggtheme = theme_bw()) +
    facet_grid(name ~ Treatment, labeller = "label_both")
  
  if(nrow(shapiro[shapiro$normal, ]) < 0.7 * nrow(shapiro))
    warning("Some of the data is not normally distributed")
  
  # Use the rstatix package, it's robust and fairly simple to use. See here:
  # https://www.datanovia.com/en/lessons/repeated-measures-anova-in-r/#two-way-repeated-measures-anova
  
  # First, perform 2-way repeated measures ANOVA
  res.aov <- anova_test(
    data = x, dv = value, wid = Rep,
    within = c(Treatment, name), detailed = TRUE)
  
  # Next, post-hoc analyses. The analyses will generate p-values initially;
  # these may be representative of a single pairwise t-test with no context of
  # the rest of the dataset. The bonferroni adjustment will take into account 
  # the whole dataset. In this case, there for pairwise treatment tests, there
  # are 21 different combinations of treatment comparisons. A standard pairwise
  # t-test might wind up with a p-value of 0.03; however, when adjusted, the 
  # p-value becomes 0.03 * 21 (taking into account the groupings) = 0.63, making
  # that comparison non-significant.
  
  # Effect of treatment at each time point
  one_way_treatment <- group_by(x, name) %>%
    anova_test(dv = value, wid = Rep, within = Treatment) %>%
    get_anova_table() %>%
    adjust_pvalue(method = "bonferroni")
  # Pairwise comparisons between treatment groups
  pwc_treatment <- group_by(x, name) %>%
    pairwise_t_test(
      value ~ Treatment, paired = TRUE,
      p.adjust.method = "bonferroni")
  
  # Effect of time at each level of treatment
  one_way_time <- group_by(x, Treatment) %>%
    anova_test(dv = value, wid = Rep, within = name) %>%
    get_anova_table() %>%
    adjust_pvalue(method = "bonferroni")
  # Pairwise comparisons between time points
  pwc_time <- group_by(x, Treatment) %>%
    pairwise_t_test(
      value ~ name, paired = TRUE,
      p.adjust.method = "bonferroni") %>% 
    add_xy_position(x = "Treatment", fun = "mean_sd")
  
  return(list(
    outliers = outliers, shapiro = shapiro, norm_plot = norm_plot, 
    mauchly = res.aov$`Mauchly's Test for Sphericity`, anova = res.aov$ANOVA, 
    treatment_effect = one_way_treatment, time_effect = one_way_time, 
    pairwise_treatment = pwc_treatment, pairwise_time = pwc_time))})

# Write outputs
for(i in names(rep_anova[[1]])[!names(rep_anova[[1]]) %in% "norm_plot"]) {
  j <- lapply(rep_anova, "[[", i)
  for(k in names(j)) {
    file_out <- file.path(results_dir, paste0("repeated_measures_", k, ".csv"))
    if(i == names(rep_anova[[1]])[1]) file.remove(file_out)
    l <- j[[k]]
    l$table_id <- i
    l[nrow(l) + 1, ] <- NA
    write.table(l, file_out, append = TRUE, sep = ",", 
                na = "", row.names = FALSE)}}

```

Moving on to the plots where lines are drawn between individuals indicating which direction they changed.

```{r Pairwise plots between time points}

plots <- lapply(ls, function(x) {
  df <- pivot_wider(x, names_from = "name") %>% 
    dplyr::mutate(Treatment = gsub("ASH", "Ash", .$Treatment)) %>% 
    dplyr::mutate(Treatment = gsub("CONTROL", "Control", .$Treatment))
  p <- ggpaired(df, cond1 = ifelse(any(grepl("Leaves", x$name)), 
                                   "Planting_Leaves", "Planting_Height"), 
                cond2 = ifelse(any(grepl("Leaves", x$name)), 
                               "Harvest_Leaves", "Harvest_Height"), nrow = 1,
                facet.by = "Treatment", fill = "condition", ggtheme = theme_classic(),
                palette = c("white", "grey33"), width = 0.85, line.color = "grey66", 
                line.size = 0.5, linetype = "solid", xlab = "Treatment", 
                ylab = ifelse(any(grepl("Leaves", x$name)), "No. Leaves", "Leaf Height (cm)")) +
    theme_classic(base_size = 14) +
    theme(
      rect = element_rect(fill = "transparent"),
      text = element_text(family = "Times New Roman"),
      legend.position = "bottom",
      legend.background = element_rect(
        fill = "white",
        size = 1,
        linetype = "solid",
        colour = "black"),
      legend.title = element_text(face = "bold", size = 14),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      panel.spacing = unit(0, "lines"),
      axis.text = element_text(colour = "black"),
      axis.text.x = element_blank(),
      axis.ticks.x = element_blank(),
      axis.ticks.y = element_line(size = 1, linetype = "solid", colour = "black"),
      axis.title = element_text(face = "bold", size = 14),
      axis.title.x = element_blank(),
      axis.title.y = element_text(margin = margin(t = 0, 20, 0, 0)),
      axis.line = element_blank(),
      panel.border = element_rect(colour = "black", size = 2),
      strip.background = element_rect(colour = "black", size = 2),
      strip.text = element_text(face = "bold", size = 12),
      plot.title = element_text(face = "bold", hjust = 0.5)
    ) + 
    scale_y_continuous(
      expand = c(0, 0), 
      limits = c(0, ifelse(any(grepl("Leaves", x$name)), 75, 20)), 
      breaks = unlist(ifelse(any(grepl("Leaves", x$name)), list(c(0, 25, 50, 75)), 
                             list(c(0, 5, 10, 15, 20))))) + 
    scale_fill_manual(breaks = c(unlist(
      ifelse(any(grepl("Leaves", x$name)), list(c("Planting_Leaves", "Harvest_Leaves")),
             list(c("Planting_Height", "Harvest_Height"))))),
      labels = c("May 1, 2018", "July 30, 2018"),
      name = "Sampling Date:", values = c("white", "gray33"))
})

n_leaves_and_height <- ggarrange(plotlist = plots[1:2], common.legend = TRUE, legend = "bottom",
                                 nrow = 2) + 
  ggsave(file.path(plot_dir, "leaf_data.png"), width = 6, height = 4, dpi = 300)

```

Identify some broad trends with the data. First, try to find trends using the whole dataset. After, filter the data to only include plants that survived and then run similar trend analyses.

```{r Summarized data}

field_sum <- group_by(field, Treatment) %>% 
  group_by(Treatment) %>% 
  add_tally() %>% 
  dplyr::summarise(across(everything(), c(mean, sd)), .groups = "drop") %>% 
  dplyr::mutate(across(ends_with("_2"), ~ .x / sqrt(n_1)))

field_sum_nozero <- dplyr::filter(field, Harvest_Height > 0 | Harvest_Leaves > 0) %>% 
  group_by(Treatment) %>% 
  add_tally() %>% 
  dplyr::summarise(across(everything(), c(mean, sd)), .groups = "drop") %>% 
  dplyr::mutate(across(ends_with("_2"), ~ .x / sqrt(n_1)))

names(field_sum) <- gsub("_2", "_se", names(field_sum))
names(field_sum) <- gsub("_1", "_mean", names(field_sum))
names(field_sum_nozero) <- gsub("_2", "_se", names(field_sum_nozero))
names(field_sum_nozero) <- gsub("_1", "_mean", names(field_sum_nozero))

write.csv(field_sum_nozero, file.path(results_dir, "field_summary_nozero.csv"), row.names = FALSE)
write.csv(field_sum, file.path(results_dir, "field_summary.csv"), row.names = FALSE)

## Extra stuff
library(weathercan)
library(sf)
sample_sites <- st_read("./01_Spatial_Data/results/study_site_coordinates.csv",
                        crs = 3005, 
                        options = c("X_POSSIBLE_NAMES=X","Y_POSSIBLE_NAMES=Y"))
center <- st_coordinates(st_transform(st_centroid(st_as_sfc(st_bbox(sample_sites))), 4326))
stations <- unique(stations_search(coords = c(center[2], center[1]))$station_id)
weather <- weather_dl(stations, start = "2018-05-01", end = "2018-07-30")

hot_day <- weather[which.max(weather$temp), ]
cold_day <- weather[which.min(weather$temp), ]

temp_high <- hot_day$temp
temp_low <- cold_day$temp

```
